# **Fundus Image Anomaly Detection for Quality Control**

This project uses an **autoencoder**, a type of deep learning model, to distinguish between 'good' and 'poor' quality fundus images. This is an **anomaly detection** task, where the model learns what a 'good' image looks like and then flags images that deviate from this norm as anomalies ('poor' quality).

This approach is used because the training dataset is expected to contain **only good quality images**.

## **Table of Contents**

1. [How it Works: Autoencoders](https://www.google.com/search?q=%23how-it-works-autoencoders)  
2. [Project Structure](https://www.google.com/search?q=%23project-structure)  
3. [Setup and Installation](https://www.google.com/search?q=%23setup-and-installation)  
4. [Dataset Preparation](https://www.google.com/search?q=%23dataset-preparation)  
5. [How to Run](https://www.google.com/search?q=%23how-to-run)  
6. [Model Architecture](https://www.google.com/search?q=%23model-architecture)

### **1\. How it Works: Autoencoders**

An autoencoder is trained to reconstruct its own input. It has two parts:

* **Encoder**: Compresses the input image into a small, dense representation (bottleneck).  
* **Decoder**: Tries to reconstruct the original image from this compressed representation.

We train the autoencoder *only on good quality images*. As a result, it becomes very proficient at reconstructing them, leading to a low **reconstruction error** (the difference between the original and reconstructed image).

When a **poor quality image** (an anomaly) is fed into the model, it struggles to reconstruct it properly because it has never seen such features during training. This results in a **high reconstruction error**.

We calculate a **threshold** based on the errors from good images. Any new image with an error above this threshold is classified as 'poor'.

### **2\. Project Structure**

The script will now automatically create the train\_good and validation\_good folders. You only need to provide the source images.

.  
├── fundus\_images/  
│   ├── all\_good\_images/      \<-- PUT ALL YOUR GOOD IMAGES HERE  
│   │   ├── good\_image\_01.jpg  
│   │   ├── good\_image\_02.png  
│   │   └── ...  
│   │  
│   ├── test/  (Optional, but highly recommended for evaluation)  
│   │   ├── good/  
│   │   │   └── ...  
│   │   └── poor/  
│   │       └── ...  
│   │  
│   ├── train\_good/           (Auto-generated by the script)  
│   └── validation\_good/      (Auto-generated by the script)  
│  
├── image\_classifier.py  
├── fundus\_autoencoder.keras  (This will be generated after training)  
├── README.md  
└── .gitignore

### **3\. Setup and Installation**

Follow these steps to set up the environment required to run the script.

**a. Create a Virtual Environment (Recommended)**

\# For Python 3  
python3 \-m venv venv  
source venv/bin/activate  \# On Windows use \`venv\\Scripts\\activate\`

**b. Install Dependencies**

Install the necessary Python libraries using pip.

pip install tensorflow matplotlib numpy

### **4\. Dataset Preparation**

Your setup process is now much simpler.

1. **Create the Source Directory**:  
   * Create a folder named fundus\_images.  
   * Inside it, create another folder named all\_good\_images.  
2. **Add Your Images**:  
   * Place **all** of your good quality fundus images directly inside the fundus\_images/all\_good\_images/ directory. The script will handle the rest.  
3. **Create a Test Directory (Optional but Recommended)**:  
   * To see how well your model performs, create a test directory inside fundus\_images.  
   * Inside test, create two folders: good and poor.  
   * Populate these with a small set of images the model has **never seen before**. This will be used to calculate a final accuracy score.

### **5\. How to Run**

Once your environment is set up and your data is organized, start the training process from your terminal:

python image\_classifier.py

The script will first split your data into training and validation sets, then:

1. Print a summary of the autoencoder architecture.  
2. Train the model, showing the reconstruction loss for each epoch.  
3. Display a plot of the training vs. validation loss.  
4. Calculate and print the **reconstruction error threshold** that will be used for classification.  
5. Show a visual comparison of some original validation images and their reconstructions.  
6. If you created the test directory, it will evaluate the model on it and print a final accuracy.  
7. Save the trained autoencoder to fundus\_autoencoder.keras.

### **6\. Model Architecture**

The model is a convolutional autoencoder.

* **Encoder**: Consists of three Conv2D and MaxPooling2D blocks that progressively downsample the image from 128x128 to 16x16, increasing the number of feature channels (32 \-\> 64 \-\> 128).  
* **Decoder**: Mirrors the encoder's structure using UpSampling2D and Conv2D layers to reconstruct the image back to its original dimensions.  
* **Loss Function**: The model is optimized using mean\_squared\_error to minimize the pixel-wise difference between the input and the output.
